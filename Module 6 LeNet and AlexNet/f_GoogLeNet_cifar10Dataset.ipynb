{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmSk4pL2ZEiX",
        "outputId": "3cfc4bcd-a82e-47c0-ed88-872a2c0d4b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess and augment the data\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Flatten, Dense\n",
        "\n",
        "# Define the Inception Module\n",
        "def inception_module(x, filters):\n",
        "    conv1x1 = Conv2D(filters[0], (1, 1), padding='same', activation='relu')(x)\n",
        "\n",
        "    conv3x3_reduce = Conv2D(filters[1], (1, 1), padding='same', activation='relu')(x)\n",
        "    conv3x3 = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(conv3x3_reduce)\n",
        "\n",
        "    conv5x5_reduce = Conv2D(filters[3], (1, 1), padding='same', activation='relu')(x)\n",
        "    conv5x5 = Conv2D(filters[4], (5, 5), padding='same', activation='relu')(conv5x5_reduce)\n",
        "\n",
        "    maxpool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    maxpool_proj = Conv2D(filters[5], (1, 1), padding='same', activation='relu')(maxpool)\n",
        "\n",
        "    inception_output = concatenate([conv1x1, conv3x3, conv5x5, maxpool_proj], axis=-1)\n",
        "    return inception_output\n",
        "\n",
        "# Define the GoogLeNet model\n",
        "def googlenet(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Initial Convolution and MaxPooling\n",
        "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(input_layer)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Inception Modules\n",
        "    x = inception_module(x, [64, 64, 128, 32, 32, 32])\n",
        "    x = inception_module(x, [128, 128, 192, 96, 96, 64])\n",
        "\n",
        "    # Add more inception modules as needed\n",
        "\n",
        "    # Auxiliary Classifier 1\n",
        "    aux1 = AveragePooling2D((5, 5), strides=(3, 3))(x)\n",
        "    aux1 = Conv2D(128, (1, 1), padding='same', activation='relu')(aux1)\n",
        "    aux1 = Flatten()(aux1)\n",
        "    aux1 = Dense(1024, activation='relu')(aux1)\n",
        "    aux1 = Dense(num_classes, activation='softmax')(aux1)\n",
        "\n",
        "    # Inception Modules and other layers\n",
        "\n",
        "    # Auxiliary Classifier 2\n",
        "    aux2 = AveragePooling2D((5, 5), strides=(3, 3))(x)\n",
        "    aux2 = Conv2D(128, (1, 1), padding='same', activation='relu')(aux2)\n",
        "    aux2 = Flatten()(aux2)\n",
        "    aux2 = Dense(1024, activation='relu')(aux2)\n",
        "    aux2 = Dense(num_classes, activation='softmax')(aux2)\n",
        "\n",
        "    # Main Classifier\n",
        "    x = AveragePooling2D((7, 7), strides=(1, 1))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1000, activation='relu')(x)  # Adjust the number of neurons for your specific problem\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_layer, outputs=[x, aux1, aux2])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the GoogLeNet model\n",
        "input_shape = (224, 224, 3)  # Adjust the input shape according to your dataset\n",
        "num_classes = 1000  # Adjust the number of classes for your specific problem\n",
        "model = googlenet(input_shape, num_classes)\n",
        "\n",
        "# Print a summary of the model\n",
        "model. Summary()\n"
      ],
      "metadata": {
        "id": "ZYo5UkE8ZMAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auxilary Classifiers\n",
        "#Before bringing the exploration of the GoogLeNet architecture to a close, thereâ€™s one more component that was implemented by the creators of the network\n",
        "#to regularise and prevent overfitting. This additional component is known as an Auxilary Classifier.\n",
        "\n",
        "#One main problem of an extensive network is that they suffer from vanishing gradient descent. Vanishing gradient descent occurs when the update to the\n",
        "#weights that arises from backpropagation is negligible within the bottom layers as a result of relatively small gradient value. Simply kept, the network stops\n",
        "#learning during training.\n",
        "\n",
        "#Auxilary Classifiers are added to the intermediate layers of the architecture, namely the third(Inception 4[a]) and sixth (Inception4[d]).\n",
        "\n",
        "#Auxilary Classifiers are only utilised during training and removed during inference. The purpose of an auxiliary classifier is to perform a\n",
        "#classification based on the inputs within the network's midsection and add the loss calculated during the training back to the total loss of the network.The loss of\n",
        "#auxiliary classifiers is weighted, meaning the calculated loss is multiplied by 0.3."
      ],
      "metadata": {
        "id": "glFWS9Ifagkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "oKyDoRHnZ7FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    datagen.flow(x_train, y_train, batch_size=64),\n",
        "    steps_per_epoch=len(x_train) // 64,\n",
        "    epochs=100,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "WXEPLa9HaBtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gbgr5BVfaGzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGN3hg-3aJkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MAhm1bCtaF_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "SAFDD_ScaK-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}